{"cells":[{"cell_type":"markdown","metadata":{},"source":["# <center> Machine Learning </center>\n","# <center> Multilayer Perceptron from Scratch </center>"]},{"cell_type":"markdown","metadata":{},"source":["# About this notebook\n","\n","This notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n","\n","Contact me: https://www.linkedin.com/in/vitorgamalemos/\n","\n","**Other noteboks about neural networks:** \n","  - Simple Perceptron: https://www.kaggle.com/vitorgamalemos/neural-network-01-simple-perceptron\n","  - Multilayer Perceptron: https://www.kaggle.com/vitorgamalemos/neural-network-02-multilayer-perceptron\n","  - Convolutional neural network: https://www.kaggle.com/vitorgamalemos/object-recognition-using-convolutional-network\n","  - GANs: https://www.kaggle.com/vitorgamalemos/generating-digits-with-gans\n","  \n","  \n","\n","# 1. Introduction about Iris Flower \n","\n","<p style=\"text-align: justify;\">The Iris Flower Dataset, also called Fisherâ€™s Iris, is a dataset introduced by Ronald Fisher, a British statistician, and biologist, with several contributions to science. Ronald Fisher has well known worldwide for his paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It was in this paper that Ronald Fisher introduced the Iris flower dataset.</p>\n","\n","<p style=\"text-align: justify;\">The iris database consists of 50 samples distributed among three different species of iris. Each of these samples has specific characteristics, which allows them to be classified into three categories: Iris Setosa, Iris Virginica, and Iris versicolor. In this tutorial, we will use multilayer perceptron to separate and classify the iris samples.</p>\n","\n","- The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor).\n","\n","- Four features were measured from each sample, the length and the width of the sepals and petals, in centimeters.\n","\n","- Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n","\n","For this example, I will implement a multilayer perceptron without any Python libraries. However, to help us format and manipulate the iris data set, we will use numpy, matplotlib, seaborn, and scikit-learn libraries."]},{"cell_type":"markdown","metadata":{},"source":["# 2. Artificial Neural Networks\n","\n","<p style=\"text-align: justify;\">Artificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes. </p>\n"]},{"cell_type":"markdown","metadata":{},"source":["# 3. How implement a Multilayer Perceptron"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1. Some Python Libraries \n","\n","<p style=\"text-align: justify;\">In the first place, Let's define some libraries to help us in the manipulation the data set, such as `numpy`, `matplotlib`, `seaborn` and `scikit-learn`. In this tutorial, I am implementing a Multilayer Perceptron without any framework like Keras or similar ones. The goal here is to be as simple as possible! So to help you with this task, we implementing the neural network without using ready-made libraries. You can use numpy to work with array operations! There is no problem it! </p>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:31.559173Z","iopub.status.busy":"2021-11-10T06:29:31.558778Z","iopub.status.idle":"2021-11-10T06:29:31.589735Z","shell.execute_reply":"2021-11-10T06:29:31.58905Z","shell.execute_reply.started":"2021-11-10T06:29:31.559098Z"},"trusted":true},"outputs":[],"source":["import random\n","import seaborn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","\n","seaborn.set(style='whitegrid'); seaborn.set_context('talk')\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","from sklearn.datasets import load_iris\n","iris_data = load_iris()"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2. An analysis about the Iris Flower Dataset\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:31.592006Z","iopub.status.busy":"2021-11-10T06:29:31.591482Z","iopub.status.idle":"2021-11-10T06:29:31.597071Z","shell.execute_reply":"2021-11-10T06:29:31.596421Z","shell.execute_reply.started":"2021-11-10T06:29:31.59195Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[".. _iris_dataset:\n","\n","Iris plants dataset\n","--------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 150 (50 in each of three classes)\n","    :Number of Attributes: 4 numeric, predictive attributes and the class\n","    :Attribute Information:\n","        - sepal length in cm\n","        - sepal width in cm\n","        - petal length in cm\n","        - petal width in cm\n","        - class:\n","                - Iris-Setosa\n","                - Iris-Versicolour\n","                - Iris-Virginica\n","                \n","    :Summary Statistics:\n","\n","    ============== ==== ==== ======= ===== ====================\n","                    Min  Max   Mean    SD   Class Correlation\n","    ============== ==== ==== ======= ===== ====================\n","    sepal length:   4.3  7.9   5.84   0.83    0.7826\n","    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n","    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n","    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n","    ============== ==== ==== ======= ===== ====================\n","\n","    :Missing Attribute Values: None\n","    :Class Distribution: 33.3% for each of 3 classes.\n","    :Creator: R.A. Fisher\n","    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n","    :Date: July, 1988\n","\n","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n","from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n","Machine Learning Repository, which has two wrong data points.\n","\n","This is perhaps the best known database to be found in the\n","pattern recognition literature.  Fisher's paper is a classic in the field and\n","is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n","data set contains 3 classes of 50 instances each, where each class refers to a\n","type of iris plant.  One class is linearly separable from the other 2; the\n","latter are NOT linearly separable from each other.\n","\n",".. topic:: References\n","\n","   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n","     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n","     Mathematical Statistics\" (John Wiley, NY, 1950).\n","   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n","     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n","   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n","     Structure and Classification Rule for Recognition in Partially Exposed\n","     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n","     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n","   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n","     on Information Theory, May 1972, 431-433.\n","   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n","     conceptual clustering system finds 3 classes in the data.\n","   - Many, many more ...\n"]}],"source":["print(iris_data['DESCR'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:31.598939Z","iopub.status.busy":"2021-11-10T06:29:31.598507Z","iopub.status.idle":"2021-11-10T06:29:32.089302Z","shell.execute_reply":"2021-11-10T06:29:32.088419Z","shell.execute_reply.started":"2021-11-10T06:29:31.598898Z"},"trusted":true},"outputs":[],"source":["n_samples, n_features = iris_data.data.shape\n","\n","plt.subplot(1, 2, 1)\n","scatter_plot = plt.scatter(iris_data.data[:,0], iris_data.data[:,1], alpha=0.5, \n","                           c=iris_data.target) \n","plt.colorbar(ticks=([0, 1, 2]))\n","plt.title('Sepal Sample')\n","\n","plt.subplot(1, 2, 2)\n","scatter_plot_2 = plt.scatter(iris_data.data[:,2], iris_data.data[:,3], alpha=0.5, \n","                           c=iris_data.target)\n","plt.colorbar(ticks=([0, 1, 2]))\n","plt.title('Petal Sample')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:32.091334Z","iopub.status.busy":"2021-11-10T06:29:32.090807Z","iopub.status.idle":"2021-11-10T06:29:34.599962Z","shell.execute_reply":"2021-11-10T06:29:34.599291Z","shell.execute_reply.started":"2021-11-10T06:29:32.091276Z"},"trusted":true},"outputs":[],"source":["import pandas\n","from pandas.plotting import scatter_matrix\n","\n","\n","dataset = pandas.read_csv('../input/iris/Iris.csv')\n","scatter_matrix(dataset, alpha=0.5, figsize=(20, 20))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:34.602776Z","iopub.status.busy":"2021-11-10T06:29:34.602409Z","iopub.status.idle":"2021-11-10T06:29:36.307772Z","shell.execute_reply":"2021-11-10T06:29:36.306932Z","shell.execute_reply.started":"2021-11-10T06:29:34.602737Z"},"trusted":true},"outputs":[],"source":["dataset.hist(alpha=0.5, figsize=(20, 20), color='red')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:36.310194Z","iopub.status.busy":"2021-11-10T06:29:36.309902Z","iopub.status.idle":"2021-11-10T06:29:37.184075Z","shell.execute_reply":"2021-11-10T06:29:37.182884Z","shell.execute_reply.started":"2021-11-10T06:29:36.31014Z"},"trusted":true},"outputs":[],"source":["dataset.plot(subplots=True, figsize=(10, 10), sharex=False, sharey=False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Manually separating our dataset\n","\n","It is here that we will select our samples to train and test the algorithms: **80% Training Samples and 20% Test**\n","<div class=\"container-fluid\">\n","  <div class=\"row\">\n","      <div class=\"col-md-2\" align='center'>\n","      </div>\n","      <div class='col-md-8' align='center'>\n","      </div>\n","      <div class=\"col-md-2\" align='center'></div>\n","  </div>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.186181Z","iopub.status.busy":"2021-11-10T06:29:37.185819Z","iopub.status.idle":"2021-11-10T06:29:37.202464Z","shell.execute_reply":"2021-11-10T06:29:37.201397Z","shell.execute_reply.started":"2021-11-10T06:29:37.186122Z"},"trusted":true},"outputs":[],"source":["random.seed(123)\n","\n","def separate_data():\n","    A = iris_dataset[0:40]\n","    tA = iris_dataset[40:50]\n","    B = iris_dataset[50:90]\n","    tB = iris_dataset[90:100]\n","    C = iris_dataset[100:140]\n","    tC = iris_dataset[140:150]\n","    train = np.concatenate((A,B,C))\n","    test =  np.concatenate((tA,tB,tC))\n","    return train,test\n","\n","train_porcent = 80 # Porcent Training \n","test_porcent = 20 # Porcent Test\n","iris_dataset = np.column_stack((iris_data.data,iris_data.target.T)) #Join X and Y\n","iris_dataset = list(iris_dataset)\n","random.shuffle(iris_dataset)\n","\n","Filetrain, Filetest = separate_data()\n","\n","train_X = np.array([i[:4] for i in Filetrain])\n","train_y = np.array([i[4] for i in Filetrain])\n","test_X = np.array([i[:4] for i in Filetest])\n","test_y = np.array([i[4] for i in Filetest])"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1. Plot our training Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.205075Z","iopub.status.busy":"2021-11-10T06:29:37.204498Z","iopub.status.idle":"2021-11-10T06:29:37.579864Z","shell.execute_reply":"2021-11-10T06:29:37.578484Z","shell.execute_reply.started":"2021-11-10T06:29:37.204865Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","\n","\n","plt.subplot(1, 2, 1)\n","plt.scatter(train_X[:,0],train_X[:,1],c=train_y,cmap=cm.viridis)\n","plt.xlabel(iris_data.feature_names[0])\n","plt.ylabel(iris_data.feature_names[1])\n","\n","plt.subplot(1, 2, 2)\n","plt.scatter(train_X[:,2],train_X[:,3],c=train_y,cmap=cm.viridis)\n","plt.xlabel(iris_data.feature_names[2])\n","plt.ylabel(iris_data.feature_names[3])"]},{"cell_type":"markdown","metadata":{},"source":["## 4.2. Plot our test Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.582002Z","iopub.status.busy":"2021-11-10T06:29:37.581463Z","iopub.status.idle":"2021-11-10T06:29:37.915608Z","shell.execute_reply":"2021-11-10T06:29:37.914745Z","shell.execute_reply.started":"2021-11-10T06:29:37.581949Z"},"trusted":true},"outputs":[],"source":["plt.subplot(1, 2, 1)\n","plt.scatter(test_X[:,0],test_X[:,1],c=test_y,cmap=cm.viridis)\n","plt.xlabel(iris_data.feature_names[0])\n","plt.ylabel(iris_data.feature_names[1]) \n","\n","plt.subplot(1, 2, 2)\n","plt.scatter(test_X[:,2],test_X[:,3],c=test_y,cmap=cm.viridis)\n","plt.xlabel(iris_data.feature_names[2])\n","plt.ylabel(iris_data.feature_names[3])"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","# 5. Multilayer Perceptron\n","\n","<p style=\"text-align: justify;\">Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, \"they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images\".</p>\n","\n","<p style=\"text-align: justify;\">They have found most use in applications difficult to express in a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.</p>\n","\n","<p style=\"text-align: justify;\"> More information here: [Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network)</p>\n","\n","<img src=\"https://miro.medium.com/max/1072/1*DOkHU_dgXMCybA6WWXrp4g.gif\"/>\n","\n","<p style=\"text-align: justify;\">The Multilayer Perceptron Networks are characterized by the presence of many intermediate layers (hidden) in your structure, located between input layer and output layer. With this, such networks have the advantage of being able to classify more than two different classes and It also solve non-linearly separable problems.</p>\n","<div class=\"container-fluid\"><div class=\"row\">\n","      <div class=\"col-md-2\" align='center'></div>\n","      <div class='col-md-8' align='center'>\n","           <img src='http://ffden-2.phys.uaf.edu/212_fall2003.web.dir/Keith_Palchikoff/multilayer%20perceptron.JPG' />\n","      </div><div class=\"col-md-2\" align='center'></div>\n","  </div>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["## 5.1. How does Multilayer Perceptron work? \n","\n","<p style=\"text-align: justify;\"> We can summarize the operation of the perceptron as follows it:</p>\n","\n","  - **Step 1**: Initialize the weights and bias with small-randomized values;\n","  - **Step 2**: Propagate all values in the input layer until output layer(Forward Propagation)\n","  - **Step 3**: Update weight and bias in the inner layers(Backpropagation)\n","  - **Step 4**: Do it until that the stop criterion is satisfied !\n","  \n","### Step 1: Forward propagation Algorithm\n","<img src=\"https://sebastianraschka.com/images/faq/visual-backpropagation/forward-propagation.png\">\n","\n","In order to proceed we need to improve the notation we have been using. That for, for each layer $1\\geq l\\geq L$, the activations and outputs are calculated as:\n","\n","$$\n","\\text{L}^l_j = {\\sum_i w^l_{ji} x^l_i\\, = w^l_{j,0} x^l_0 + w^l_{j,1} x^l_1 + w^l_{j,2} x^l_2 + ... + w^l_{j,n}} x^l_n,\n","$$\n","$$Y^l_j = g^l(\\text{L}^l_j)\\,,$$\n","\n","$$\\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}$$\n","\n","where:\n","\n","* $y^l_j$ is the $j-$th output of layer $l$,\n","* $x^l_i$ is the $i$-th input to layer $l$,\n","* $w^l_{ji}$ is the weight of the $j$-th neuron connected to input $i$,\n","* $\\text{L}^l_{j}$ is called net activation, and\n","* $g^l(\\cdot)$ is the activation function of layer $l$."]},{"cell_type":"markdown","metadata":{},"source":["### Step 2: Calculation our Erro function \n","<img src=\"https://miro.medium.com/max/920/1*jYQYuHpHdkZqNFQKJSuDTw.png\">\n","It is used to measure performance locality associated with the results produced by the neurons in output layer and the expected result.\n","$$\n","E(k) = \n","\\frac{1}{2} \\sum_{k=1}^{K}({{d_j(k)}} - {y_j}{(k)})^2.\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3. Activation Functions\n","<img src=\"https://miro.medium.com/max/1192/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\">\n","\n","### Sigmoid Function:"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.917547Z","iopub.status.busy":"2021-11-10T06:29:37.916988Z","iopub.status.idle":"2021-11-10T06:29:37.922498Z","shell.execute_reply":"2021-11-10T06:29:37.921701Z","shell.execute_reply.started":"2021-11-10T06:29:37.917225Z"},"trusted":true},"outputs":[],"source":["x = 0 \n","ativation = {(lambda x: 1/(1 + np.exp(-x)))}\n","deriv = {(lambda x: x*(1-x))}"]},{"cell_type":"markdown","metadata":{},"source":[" ### Hyperbolic Tangent Function:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.924125Z","iopub.status.busy":"2021-11-10T06:29:37.923758Z","iopub.status.idle":"2021-11-10T06:29:37.935702Z","shell.execute_reply":"2021-11-10T06:29:37.934863Z","shell.execute_reply.started":"2021-11-10T06:29:37.923971Z"},"trusted":true},"outputs":[],"source":["activation_tang = {(lambda x: np.tanh(x))}\n","deriv_tang = {(lambda x: 1-x**2)}\n","  "]},{"cell_type":"markdown","metadata":{},"source":["### ReLU Function:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.937285Z","iopub.status.busy":"2021-11-10T06:29:37.937006Z","iopub.status.idle":"2021-11-10T06:29:37.94887Z","shell.execute_reply":"2021-11-10T06:29:37.948097Z","shell.execute_reply.started":"2021-11-10T06:29:37.937222Z"},"trusted":true},"outputs":[],"source":["activation_ReLU = {(lambda x: x*(x > 0))}\n","deriv_ReLU = {(lambda x: 1 * (x>0))}"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Step 4. Backpropagation Algorithm\n","<img src=\"https://sebastianraschka.com/images/faq/visual-backpropagation/backpropagation.png\">\n","### In Output Layer,  $L = 2:$\n","   - **Step 1**: Calculate error in output layer: $\\delta^{(L2)} = -({d_j}^{(L2)} - {y_j}^{(L2)})\\cdot\n","   g'({S_j}^{(L2)})$\n","   \n","      `\n","      ERROR_output = self.OUTPUT - self.OUTPUT_L2\n","      DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n","      `\n","      \n","\n","   - **Step 2**: Update all weight between hidden and output layer: $W^{(L2)} = W^{(L2)} -\\gamma \\cdot(\\delta^{(L2)}  - {S_j}^{(L1)})$\n","   \n","         for i in range(self.hiddenLayer):`\n","           ` for j in range(self.OutputLayer):`\n","               ` self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.output_l1[i]))`\n","               ` self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])`\n","               \n","   - **Step 3**: Update bias value in output layer: $bias^{(L2)} = bias^{(L2)} - \\gamma \\cdot \\delta^{(L2)}$\n","   \n","### In Input Layer , $L = 1$:\n","   - **Step 4**: Calculate error in hidden layer: $\\delta^{(L1)} = W^{(L2)} \\cdot \\delta^{(L2)} \\cdot g'({S_j}^{(L1)})$\n","     \n","   `delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output) * self.deriv(self._l1)`\n","   - **Step 5**: Update all weight between hidden and output layer: $W^{(L1)} = W^{(L1)} -\\gamma \\cdot(\\delta^{(L1)}  - {X_i})$\n","         `for i in range(self.OutputLayer):`\n","           `for j in range(self.hiddenLayer):`\n","               `self.WEIGHT_hidden[i][j] -= (self.learningRate * (DELTA_hidden[j] * INPUT[i]))`\n","               `self.BIAS_hidden[j] -= (self.learningRate * DELTA_hidden[j])`\n","   - **Step 6**: Update bias value in output layer: $bias^{(L1)} = bias^{(L1)} - \\gamma \\cdot \\delta^{(L1)}$"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://thumbs.gfycat.com/FickleHorribleBlackfootedferret-small.gif\">"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Implementation the Multilayer Perceptron in Python\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:37.950695Z","iopub.status.busy":"2021-11-10T06:29:37.950411Z","iopub.status.idle":"2021-11-10T06:29:38.003176Z","shell.execute_reply":"2021-11-10T06:29:38.002462Z","shell.execute_reply.started":"2021-11-10T06:29:37.950644Z"},"trusted":true},"outputs":[],"source":["from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n","import random\n","\n","class MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n","    def __init__(self, params=None):     \n","        if (params == None):\n","            self.inputLayer = 4                        # Input Layer\n","            self.hiddenLayer = 5                       # Hidden Layer\n","            self.outputLayer = 3                       # Outpuy Layer\n","            self.learningRate = 0.005                  # Learning rate\n","            self.max_epochs = 600                      # Epochs\n","            self.iasHiddenValue = -1                   # Bias HiddenLayer\n","            self.BiasOutputValue = -1                  # Bias OutputLayer\n","            self.activation = self.ativacao['sigmoid'] # Activation function\n","            self.deriv = self.derivada['sigmoid']\n","        else:\n","            self.inputLayer = params['InputLayer']\n","            self.hiddenLayer = params['HiddenLayer']\n","            self.OutputLayer = params['OutputLayer']\n","            self.learningRate = params['LearningRate']\n","            self.max_epochs = params['Epocas']\n","            self.BiasHiddenValue = params['BiasHiddenValue']\n","            self.BiasOutputValue = params['BiasOutputValue']\n","            self.activation = self.ativacao[params['ActivationFunction']]\n","            self.deriv = self.derivada[params['ActivationFunction']]\n","        \n","        'Starting Bias and Weights'\n","        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n","        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n","        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n","        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n","        self.classes_number = 3 \n","        \n","    pass\n","    \n","    def starting_weights(self, x, y):\n","        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n","\n","    ativacao = {\n","         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n","            'tanh': (lambda x: np.tanh(x)),\n","            'Relu': (lambda x: x*(x > 0)),\n","               }\n","    derivada = {\n","         'sigmoid': (lambda x: x*(1-x)),\n","            'tanh': (lambda x: 1-x**2),\n","            'Relu': (lambda x: 1 * (x>0))\n","               }\n"," \n","    def Backpropagation_Algorithm(self, x):\n","        DELTA_output = []\n","        'Stage 1 - Error: OutputLayer'\n","        ERROR_output = self.output - self.OUTPUT_L2\n","        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n","        \n","        arrayStore = []\n","        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n","        for i in range(self.hiddenLayer):\n","            for j in range(self.OutputLayer):\n","                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n","                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n","      \n","        'Stage 3 - Error: HiddenLayer'\n","        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n"," \n","        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n","        for i in range(self.OutputLayer):\n","            for j in range(self.hiddenLayer):\n","                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n","                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n","                \n","    def show_err_graphic(self,v_erro,v_epoca):\n","        plt.figure(figsize=(9,4))\n","        plt.plot(v_epoca, v_erro, \"m-\",color=\"b\", marker=11)\n","        plt.xlabel(\"Number of Epochs\")\n","        plt.ylabel(\"Squared error (MSE) \");\n","        plt.title(\"Error Minimization\")\n","        plt.show()\n","\n","    def predict(self, X, y):\n","        'Returns the predictions for every element of X'\n","        my_predictions = []\n","        'Forward Propagation'\n","        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n","        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n","                                 \n","        for i in forward:\n","            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])\n","            \n","        array_score = []\n","        for i in range(len(my_predictions)):\n","            if my_predictions[i] == 0: \n","                array_score.append([i, 'Iris-setosa', my_predictions[i], y[i]])\n","            elif my_predictions[i] == 1:\n","                 array_score.append([i, 'Iris-versicolour', my_predictions[i], y[i]])\n","            elif my_predictions[i] == 2:\n","                 array_score.append([i, 'Iris-virginica', my_predictions[i], y[i]])\n","                    \n","        dataframe = pd.DataFrame(array_score, columns=['_id', 'class', 'output', 'hoped_output'])\n","        return my_predictions, dataframe\n","\n","    def fit(self, X, y):  \n","        count_epoch = 1\n","        total_error = 0\n","        n = len(X); \n","        epoch_array = []\n","        error_array = []\n","        W0 = []\n","        W1 = []\n","        while(count_epoch <= self.max_epochs):\n","            for idx,inputs in enumerate(X): \n","                self.output = np.zeros(self.classes_number)\n","                'Stage 1 - (Forward Propagation)'\n","                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n","                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n","                'Stage 2 - One-Hot-Encoding'\n","                if(y[idx] == 0): \n","                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n","                elif(y[idx] == 1):\n","                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n","                elif(y[idx] == 2):\n","                    self.output = np.array([0,0,1]) #Class3 {0,0,1}\n","                \n","                square_error = 0\n","                for i in range(self.OutputLayer):\n","                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n","                    square_error = (square_error + (0.05 * erro))\n","                    total_error = total_error + square_error\n","         \n","                'Backpropagation : Update Weights'\n","                self.Backpropagation_Algorithm(inputs)\n","                \n","            total_error = (total_error / n)\n","            if((count_epoch % 50 == 0)or(count_epoch == 1)):\n","                print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n","                error_array.append(total_error)\n","                epoch_array.append(count_epoch)\n","                \n","            W0.append(self.WEIGHT_hidden)\n","            W1.append(self.WEIGHT_output)\n","             \n","                \n","            count_epoch += 1\n","        self.show_err_graphic(error_array,epoch_array)\n","        \n","        plt.plot(W0[0])\n","        plt.title('Weight Hidden update during training')\n","        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n","        plt.ylabel('Value Weight')\n","        plt.show()\n","        \n","        plt.plot(W1[0])\n","        plt.title('Weight Output update during training')\n","        plt.legend(['neuron1', 'neuron2', 'neuron3'])\n","        plt.ylabel('Value Weight')\n","        plt.show()\n","\n","        return self"]},{"cell_type":"markdown","metadata":{},"source":["## Finding the best parameters \n","\n","<p style=\"text-align: justify;\">For find the best parameters, it was necessary to realize various tests using different values to the parameters. The graphs below denote all tests made to select the best configuration for the multilayer perceptron. These tests were important in selecting the best settings and ensuring the best accuracy. The graph was drawn manually, but you can change the settings and note the results obtained. The tests involve different activation functions and the number of neurons for each layer.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:38.004777Z","iopub.status.busy":"2021-11-10T06:29:38.004368Z","iopub.status.idle":"2021-11-10T06:29:38.026027Z","shell.execute_reply":"2021-11-10T06:29:38.02514Z","shell.execute_reply.started":"2021-11-10T06:29:38.004722Z"},"trusted":true},"outputs":[],"source":["def show_test():\n","    ep1 = [0,100,200,300,400,500,600,700,800,900,1000,1500,2000]\n","    h_5 = [0,60,70,70,83.3,93.3,96.7,86.7,86.7,76.7,73.3,66.7,66.7]\n","    h_4 = [0,40,70,63.3,66.7,70,70,70,70,66.7,66.7,43.3,33.3]\n","    h_3 = [0,46.7,76.7,80,76.7,76.7,76.6,73.3,73.3,73.3,73.3,76.7,76.7]\n","    plt.figure(figsize=(10,4))\n","    l1, = plt.plot(ep1, h_3, \"--\",color='b',label=\"node-3\", marker=11)\n","    l2, = plt.plot(ep1, h_4, \"--\",color='g',label=\"node-4\", marker=8)\n","    l3, = plt.plot(ep1, h_5, \"--\",color='r',label=\"node-5\", marker=5)\n","    plt.legend(handles=[l1,l2,l3], loc=1)\n","    plt.xlabel(\"number of Epochs\")\n","    plt.ylabel(\"% Hits\")\n","    plt.title(\"Number of Hidden Layers - Performance\")\n","    \n","    ep2 = [0,100,200,300,400,500,600,700]\n","    tanh = [0.18,0.027,0.025,0.022,0.0068,0.0060,0.0057,0.00561]\n","    sigm = [0.185,0.0897,0.060,0.0396,0.0343,0.0314,0.0296,0.0281]\n","    Relu = [0.185,0.05141,0.05130,0.05127,0.05124,0.05123,0.05122,0.05121]\n","    plt.figure(figsize=(10,4))\n","    l1 , = plt.plot(ep2, tanh, \"--\",color='b',label=\"Hyperbolic Tangent\",marker=11)\n","    l2 , = plt.plot(ep2, sigm, \"--\",color='g',label=\"Sigmoide\", marker=8)\n","    l3 , = plt.plot(ep2, Relu, \"--\",color='r',label=\"ReLu\", marker=5)\n","    plt.legend(handles=[l1,l2,l3], loc=1)\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Error\")\n","    plt.title(\"Activation Functions - Performance\")\n","    \n","    fig, ax = plt.subplots()\n","    names = [\"Hyperbolic Tangent\",\"Sigmoide\",\"ReLU\"]\n","    x1 = [2.0,4.0,6.0]\n","    plt.bar(x1[0], 53.4,0.4,color='b')\n","    plt.bar(x1[1], 96.7,0.4,color='g')\n","    plt.bar(x1[2], 33.2,0.4,color='r')\n","    plt.xticks(x1,names)\n","    plt.ylabel('% Hits')\n","    plt.title('Hits - Activation Functions')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:38.027598Z","iopub.status.busy":"2021-11-10T06:29:38.027192Z","iopub.status.idle":"2021-11-10T06:29:38.759576Z","shell.execute_reply":"2021-11-10T06:29:38.75865Z","shell.execute_reply.started":"2021-11-10T06:29:38.027551Z"},"trusted":true},"outputs":[],"source":["show_test()"]},{"cell_type":"markdown","metadata":{},"source":["# Training the Artificial Neural Network(MLP)\n","\n","## Step 1: training our MultiLayer Perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:29:38.761375Z","iopub.status.busy":"2021-11-10T06:29:38.761085Z","iopub.status.idle":"2021-11-10T06:30:06.146303Z","shell.execute_reply":"2021-11-10T06:30:06.145537Z","shell.execute_reply.started":"2021-11-10T06:29:38.761324Z"},"trusted":true},"outputs":[],"source":["dictionary = {'InputLayer':4, 'HiddenLayer':5, 'OutputLayer':3,\n","              'Epocas':700, 'LearningRate':0.005,'BiasHiddenValue':-1, \n","              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid'}\n","\n","Perceptron = MultiLayerPerceptron(dictionary)\n","Perceptron.fit(train_X,train_y)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: testing our results "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:30:06.148038Z","iopub.status.busy":"2021-11-10T06:30:06.147764Z","iopub.status.idle":"2021-11-10T06:30:06.16151Z","shell.execute_reply":"2021-11-10T06:30:06.160612Z","shell.execute_reply.started":"2021-11-10T06:30:06.147988Z"},"trusted":true},"outputs":[],"source":["prev, dataframe = Perceptron.predict(test_X, test_y)\n","hits = n_set = n_vers = n_virg = 0\n","score_set = score_vers = score_virg = 0\n","for j in range(len(test_y)):\n","    if(test_y[j] == 0): n_set += 1\n","    elif(test_y[j] == 1): n_vers += 1\n","    elif(test_y[j] == 2): n_virg += 1\n","        \n","for i in range(len(test_y)):\n","    if test_y[i] == prev[i]: \n","        hits += 1\n","    if test_y[i] == prev[i] and test_y[i] == 0:\n","        score_set += 1\n","    elif test_y[i] == prev[i] and test_y[i] == 1:\n","        score_vers += 1\n","    elif test_y[i] == prev[i] and test_y[i] == 2:\n","        score_virg += 1    \n","         \n","hits = (hits / len(test_y)) * 100\n","faults = 100 - hits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:31:21.281746Z","iopub.status.busy":"2021-11-10T06:31:21.281441Z","iopub.status.idle":"2021-11-10T06:31:21.302387Z","shell.execute_reply":"2021-11-10T06:31:21.301544Z","shell.execute_reply.started":"2021-11-10T06:31:21.28171Z"},"trusted":true},"outputs":[],"source":["dataframe"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Accuracy and precision the Multilayer Perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:30:06.163448Z","iopub.status.busy":"2021-11-10T06:30:06.163131Z","iopub.status.idle":"2021-11-10T06:30:06.307556Z","shell.execute_reply":"2021-11-10T06:30:06.306541Z","shell.execute_reply.started":"2021-11-10T06:30:06.163396Z"},"trusted":true},"outputs":[],"source":["graph_hits = []\n","print(\"Porcents :\",\"%.2f\"%(hits),\"% hits\",\"and\",\"%.2f\"%(faults),\"% faults\")\n","print(\"Total samples of test\",n_samples)\n","print(\"*Iris-Setosa:\",n_set,\"samples\")\n","print(\"*Iris-Versicolour:\",n_vers,\"samples\")\n","print(\"*Iris-Virginica:\",n_virg,\"samples\")\n","\n","graph_hits.append(hits)\n","graph_hits.append(faults)\n","labels = 'Hits', 'Faults';\n","sizes = [96.5, 3.3]\n","explode = (0, 0.14)\n","\n","fig1, ax1 = plt.subplots();\n","ax1.pie(graph_hits, explode=explode,colors=['green','red'],labels=labels, autopct='%1.1f%%',\n","shadow=True, startangle=90)\n","ax1.axis('equal')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4. Score for each one of the samples"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T06:30:06.309587Z","iopub.status.busy":"2021-11-10T06:30:06.309174Z","iopub.status.idle":"2021-11-10T06:30:06.5018Z","shell.execute_reply":"2021-11-10T06:30:06.500887Z","shell.execute_reply.started":"2021-11-10T06:30:06.309515Z"},"trusted":true},"outputs":[],"source":["acc_set = (score_set/n_set)*100\n","acc_vers = (score_vers/n_vers)*100\n","acc_virg = (score_virg/n_virg)*100\n","print(\"- Acurracy Iris-Setosa:\",\"%.2f\"%acc_set, \"%\")\n","print(\"- Acurracy Iris-Versicolour:\",\"%.2f\"%acc_vers, \"%\")\n","print(\"- Acurracy Iris-Virginica:\",\"%.2f\"%acc_virg, \"%\")\n","names = [\"Setosa\",\"Versicolour\",\"Virginica\"]\n","x1 = [2.0,4.0,6.0]\n","fig, ax = plt.subplots()\n","r1 = plt.bar(x1[0], acc_set,color='orange',label='Iris-Setosa')\n","r2 = plt.bar(x1[1], acc_vers,color='green',label='Iris-Versicolour')\n","r3 = plt.bar(x1[2], acc_virg,color='purple',label='Iris-Virginica')\n","plt.ylabel('Scores %')\n","plt.xticks(x1, names);plt.title('Scores by iris flowers - Multilayer Perceptron')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.9"},"vscode":{"interpreter":{"hash":"d454fec381973a088ed59926085df1a0c593a668c15db352fb83e31ffdc2dc77"}}},"nbformat":4,"nbformat_minor":4}
